{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af9472be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf \n",
    "import tensorflow_hub as tfhub\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448fa980",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f45a6913",
   "metadata": {},
   "outputs": [],
   "source": [
    "(raw_train, raw_validation, raw_test), metadata = tfds.load(\n",
    "    'cats_vs_dogs',\n",
    "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
    "    with_info=True,\n",
    "    as_supervised=True,\n",
    ")\n",
    "\n",
    "num_examples = metadata.splits['train'].num_examples\n",
    "num_classes = metadata.features['label'].num_classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d698aea",
   "metadata": {},
   "source": [
    "### Create TensorFlow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "256e7e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_image(image, label):\n",
    "    image = tf.image.resize(image, (224, 224)) / 255.0\n",
    "    return  image, label\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_batches = raw_train.shuffle(num_examples // 4).map(format_image).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "validation_batches = raw_validation.map(format_image).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_batches = raw_test.map(format_image).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db06efd7",
   "metadata": {},
   "source": [
    "### Build Model\n",
    "[TF HUB](https://tfhub.dev/) <br>\n",
    "https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c018b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "582/582 [==============================] - 52s 77ms/step - loss: 0.0474 - accuracy: 0.9828 - val_loss: 0.0387 - val_accuracy: 0.9871\n",
      "Epoch 2/6\n",
      "582/582 [==============================] - 46s 77ms/step - loss: 0.0282 - accuracy: 0.9903 - val_loss: 0.0463 - val_accuracy: 0.9858\n",
      "Epoch 3/6\n",
      "582/582 [==============================] - 46s 78ms/step - loss: 0.0209 - accuracy: 0.9924 - val_loss: 0.0505 - val_accuracy: 0.9854\n",
      "Epoch 4/6\n",
      "582/582 [==============================] - 46s 78ms/step - loss: 0.0174 - accuracy: 0.9944 - val_loss: 0.0497 - val_accuracy: 0.9875\n",
      "Epoch 5/6\n",
      "582/582 [==============================] - 46s 78ms/step - loss: 0.0152 - accuracy: 0.9946 - val_loss: 0.0456 - val_accuracy: 0.9880\n",
      "Epoch 6/6\n",
      "582/582 [==============================] - 45s 76ms/step - loss: 0.0145 - accuracy: 0.9949 - val_loss: 0.0576 - val_accuracy: 0.9858\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tfhub.KerasLayer(\"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/5\",\n",
    "                   trainable=False),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.build([None, 224, 224, 3])\n",
    "model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "EPOCHS = 6\n",
    "hist = model.fit(train_batches,\n",
    "                 epochs=EPOCHS,\n",
    "                 validation_data=validation_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa821d49",
   "metadata": {},
   "source": [
    "### Option A: Save model --> Read and Convert TF Lite Model --> Save TF Lite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb11d60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_dir/post_training_demo/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_dir/post_training_demo/assets\n"
     ]
    }
   ],
   "source": [
    "# Save Your Model\n",
    "tf.saved_model.save(model,export_dir='model_dir/post_training_demo/')\n",
    "\n",
    "\n",
    "# Setup Converter and read TF model directory\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('model_dir/post_training_demo/')\n",
    "\n",
    "##################\n",
    "# OPTIMIZATIONS ##\n",
    "###################\n",
    "\"\"\"\n",
    "Options:\n",
    "\n",
    "OPTIMIZE_FOR_SIZE Deprecated. Does the same as DEFAULT.\n",
    "OPTIMIZE_FOR_LATENCY Deprecated. Does the same as DEFAULT.\n",
    "EXPERIMENTAL_SPARSITY Experimental flag, subject to change.\n",
    "\"\"\"\n",
    "# Find balance Between Size and Latency\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT] \n",
    "\n",
    "\"\"\"\n",
    "Full Integer Quantization or float16 Quantization\n",
    "\"\"\"\n",
    "# converter.target_spec.supported_types = [tf.float16]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "In order to quantize your model weights, \n",
    "you will need to give a sample dataset for calculations to infer the min max values \n",
    "\"\"\"\n",
    "def representative_data_generataor(num=100):\n",
    "    # allows concverter to inspect data and make optimizations selections (where to best make conversions)\n",
    "    for image, _ in test_batches.take(num):\n",
    "        yield [image]\n",
    "\n",
    "converter.representative_dataset = representative_data_generataor\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('model_dir/post_training_demo/CATS_DOGS_CLF2.tflite','wb') as file:\n",
    "    file.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01951877",
   "metadata": {},
   "source": [
    "### Option B: Direct Conversion of Keras Model to TF Lite Model (Only Possible if full model is built by tf.keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4d3898e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\user\\AppData\\Local\\Temp\\tmpon4a1y05\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\user\\AppData\\Local\\Temp\\tmpon4a1y05\\assets\n"
     ]
    }
   ],
   "source": [
    "#####################################################################################\n",
    "# Setup Converter and read TF model directory\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "#####################################################################################\n",
    "\n",
    "\n",
    "##################\n",
    "# OPTIMIZATIONS ##\n",
    "###################\n",
    "\"\"\"\n",
    "Options:\n",
    "\n",
    "OPTIMIZE_FOR_SIZE Deprecated. Does the same as DEFAULT.\n",
    "OPTIMIZE_FOR_LATENCY Deprecated. Does the same as DEFAULT.\n",
    "EXPERIMENTAL_SPARSITY Experimental flag, subject to change.\n",
    "\"\"\"\n",
    "# Find balance Between Size and Latency\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT] \n",
    "\n",
    "\"\"\"\n",
    "Full Integer Quantization or float16 Quantization\n",
    "\"\"\"\n",
    "# converter.target_spec.supported_types = [tf.float16]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "In order to quantize your model weights, \n",
    "you will need to give a sample dataset for calculations to infer the min max values \n",
    "\"\"\"\n",
    "def representative_data_generataor(num=200):\n",
    "    # allows concverter to inspect data and make optimizations selections (where to best make conversions)\n",
    "    for image, _ in test_batches.take(num):\n",
    "        yield [image]\n",
    "\n",
    "converter.representative_dataset = representative_data_generataor\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('model_dir/post_training_demo/CATS_DOGS_CLF_Keras.tflite','wb') as file:\n",
    "    file.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9badb964",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c787ca4",
   "metadata": {},
   "source": [
    "## Inferencing with TFLite Models Using Python Interpreter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fd0800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.lite.Interpreter(model_path = 'model_dir/post_training_demo/CATS_DOGS_CLF_Keras.tflite'\n",
    "                            ,num_threads=16)\n",
    "model.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed82a098",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'serving_default_keras_layer_input:0',\n",
       "  'index': 0,\n",
       "  'shape': array([  1, 224, 224,   3]),\n",
       "  'shape_signature': array([ -1, 224, 224,   3]),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_input_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa663eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'serving_default_keras_layer_input:0',\n",
       "  'index': 0,\n",
       "  'shape': array([  1, 224, 224,   3]),\n",
       "  'shape_signature': array([ -1, 224, 224,   3]),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_input_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6aa1e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "test_labels = [] \n",
    "test_imgs =  []\n",
    "\n",
    "input_index = model.get_input_details()[0][\"index\"]\n",
    "output_index = model.get_output_details()[0][\"index\"]\n",
    "\n",
    "test_batches = raw_test.map(format_image).batch(1) \n",
    "# Change BATCH_SIZE to 1 due to the require shape displayed above\n",
    "\n",
    "for img, label in test_batches.take(200):\n",
    "    # Step 1 set tensor\n",
    "    model.set_tensor(input_index, img)\n",
    "    \n",
    "    # Step 2 Run Inference\n",
    "    model.invoke()\n",
    "    pred = model.get_tensor(output_index)\n",
    "    ##\n",
    "    \n",
    "    predictions.append(pred)\n",
    "    test_labels.append(label.numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54f69427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay,confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7aafb9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_preds = []\n",
    "for res in predictions:\n",
    "    res = np.argmax(res,axis=1)\n",
    "    np_preds.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2e95213",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_labels,np_preds)\n",
    "acc = accuracy_score(test_labels,np_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "403c3151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score out of 200 samples: 98.5 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy Score out of 200 samples: {acc * 100} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f61e9bb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_plot = ConfusionMatrixDisplay(cm,display_labels=['Cats','Dogs'])\n",
    "res = cm_plot.plot(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01a658a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
